{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROYECTO COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necesarios para el correcto funcionamiento del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "import time\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primera aproximación del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite la lectura de imágenes a partir del directorio en el que estan alamacenadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lectura(subdirectorio,fichero,path=\"Dataset/\"):\n",
    "    if(os.path.exists(path+subdirectorio)):\n",
    "        listaImagenes = os.listdir(path+subdirectorio)\n",
    "        datos = []\n",
    "\n",
    "        for imagen in listaImagenes:\n",
    "            datos.append(cv2.imread(path+subdirectorio+imagen,0))\n",
    "        if(os.path.isfile(path+fichero)):\n",
    "            with open(path+fichero) as fp: \n",
    "                salidas = fp.read().splitlines()\n",
    "            return np.array(datos), np.array(salidas)\n",
    "        else:\n",
    "            print(\"¡Error! El fichero de salidas no existe\")\n",
    "    else:\n",
    "        print(\"¡Error! El directorio no existe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la funcion para leer imagenes y aumentar el tamaño de nuestro conjunto de datos, utilizando:\n",
    "- Ruido Gaussiano\n",
    "- Volteando verticalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamiento(pathImagenes=\"Data/\", dx=256, dy=256, noiseX=5, noiseY=10):\n",
    "    if(os.path.exists(pathImagenes)):\n",
    "        resultado = []\n",
    "        listaImagenes = os.listdir(pathImagenes)\n",
    "\n",
    "        #Realizar un data augmentation para aumentar los datos del dataset:\n",
    "        gaussian_noise = iaa.AdditiveGaussianNoise(noiseX, noiseY)\n",
    "        flip_vr=iaa.Fliplr(p=1.0)\n",
    "\n",
    "        for imagen in listaImagenes:\n",
    "            Xray = cv2.resize(cv2.imread(pathImagenes+imagen,0),(dx, dy))\n",
    "            resultado.append(Xray)\n",
    "            resultado.append(gaussian_noise.augment_image(Xray))\n",
    "            resultado.append(flip_vr.augment_image(Xray))\n",
    "\n",
    "        return resultado\n",
    "    else:\n",
    "        print(\"¡Error! El path especificado no ha sido encontrado\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una funcion para hacer el particionamiento de los datos en train, test y val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particionamiento(listaDatos, train_percent=.6, validate_percent=.2, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    listaTrain=[]\n",
    "    listaTest=[]\n",
    "    listaVal=[]\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    y_val = []\n",
    "    for i,lista in enumerate(listaDatos):\n",
    "        lista = np.array(lista)\n",
    "        perm = np.random.permutation(len(lista))\n",
    "        m = len(lista)\n",
    "        train_end = int(train_percent * m)\n",
    "        val_end = int(validate_percent * m) + train_end\n",
    "        train = lista[perm[:train_end]]\n",
    "        val = lista[perm[train_end:val_end]]\n",
    "        test = lista[perm[val_end:]]\n",
    "        listaTrain.extend(train)\n",
    "        listaTest.extend(test)\n",
    "        listaVal.extend(val)\n",
    "        y_train.extend(list([i]*len(train)))\n",
    "        y_test.extend(list([i]*len(test)))\n",
    "        y_val.extend(list([i]*len(val)))\n",
    "    \n",
    "    return listaTrain, listaTest, listaVal, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función permite almacenar las imágenes leidas y tratasdas durante la fase de train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def almacenaImagenes(train,test,val,y_train,y_test,y_val,path=\"Dataset/\"):\n",
    "    if(os.path.exists(path)):\n",
    "        try:\n",
    "            shutil.rmtree(path,ignore_errors=True)\n",
    "            \n",
    "        except OSError as e:\n",
    "            print(\"¡Error! No se ha podido eliminar el directorio\")\n",
    "    \n",
    "    os.mkdir(path, 0o7777)\n",
    "    os.mkdir(path+\"train/\",0o7777)\n",
    "    os.mkdir(path+\"val/\",0o7777)\n",
    "    os.mkdir(path+\"test/\",0o7777)\n",
    "    \n",
    "    f = open(path+\"train.txt\", \"w\")\n",
    "    for i,imagen in enumerate(train):\n",
    "        cv2.imwrite(path+\"train/Train-\"+str(i)+\".png\",imagen)\n",
    "        f.write(str(y_train[i])+\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    f = open(path+\"test.txt\", \"w\")\n",
    "    for i,imagen in enumerate(test):\n",
    "        cv2.imwrite(path+\"test/Test-\"+str(i)+\".png\",imagen)\n",
    "        f.write(str(y_test[i])+\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    f = open(path+\"val.txt\", \"w\")\n",
    "    for i,imagen in enumerate(val):\n",
    "        cv2.imwrite(path+\"val/Val-\"+str(i)+\".png\",imagen)\n",
    "        f.write(str(y_val[i])+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[0], norm=False):\n",
    "\n",
    "    if(norm == True):\n",
    "        Xtrain = Xtrain/255\n",
    "        Xtest = Xtest/255\n",
    "        Xval = Xval/255\n",
    "\n",
    "    accTrain=[]\n",
    "    accTest=[]\n",
    "    accVal=[]\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i,k in enumerate(score):\n",
    "        #Seleccionar aquellos pixeles que tengan un Score mayor o igual al establecido\n",
    "        pixelesSelec = df[df.Score >= k]['Pixeles'].values.tolist()\n",
    "\n",
    "        #Entrenar el modelo con los ejemplos con el nuevo numero de caracteristicas:\n",
    "        #Generar el nuevo conjunto de Train:\n",
    "        X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "\n",
    "        #Entrenar el modelo:\n",
    "        clasificador.fit(X_train_tratada,y_train)\n",
    "\n",
    "        #Obtener la predcción en train y el accuracy:\n",
    "        predictTrain = clasificador.predict(X_train_tratada)\n",
    "        accTrain.append(metrics.accuracy_score(predictTrain,y_train)*100)\n",
    "\n",
    "        #Generar el nuevo conjunto de Val:\n",
    "        X_val_tratada = Xval[:,pixelesSelec]\n",
    "\n",
    "        #Obtener la predcción en val y el accuracy:\n",
    "        predictVal = clasificador.predict(X_val_tratada)\n",
    "        accVal.append(metrics.accuracy_score(predictVal,y_val)*100)\n",
    "\n",
    "        #Generar el nuevo conjunto de Test:\n",
    "        X_test_tratada = Xtest[:,pixelesSelec]\n",
    "\n",
    "        #Obtener la predcción en test y el accuracy:\n",
    "        predictTest = clasificador.predict(X_test_tratada)\n",
    "        accTest.append(metrics.accuracy_score(predictTest,y_test)*100)\n",
    "\n",
    "        print('El rendimiento en entrenamiento con {} variables para un score de {} es de {}%'.format(len(pixelesSelec),k,accTrain[i]))\n",
    "        print('El rendimiento en validacion con {} variables para un score de {}  es de {}%'.format(len(pixelesSelec),k,accVal[i]))\n",
    "        print('El rendimiento en test con {} variables para un score de {}  es de {}%'.format(len(pixelesSelec),k,accTest[i]))\n",
    "\n",
    "    tiempo_ejecucion = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (tiempo_ejecucion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función genera la matriz de confusión para un conjunto de datos de entrada y muestra los valores de precisión, recall y f-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_confusion(X,y):\n",
    "    if(X == None or y == None):\n",
    "        print(\"¡Error en los parámetros introducidos!\")\n",
    "        return\n",
    "    \n",
    "    print(\"********************************* Plot de matriz de confusión *********************************\")\n",
    "    class_names = ['COVID-19','Normal','Viral']\n",
    "    titles_options = [(\"Matriz de confusion sin normalizar\", None),\n",
    "                      (\"Matriz de confusion normalizada\", 'true')]\n",
    "    for title, normalize in titles_options:\n",
    "        disp = metrics.plot_confusion_matrix(regresion, X_test_tratada, y_test,\n",
    "                                     display_labels=class_names,\n",
    "                                     cmap=plt.cm.Blues,\n",
    "                                     normalize=normalize)\n",
    "        disp.ax_.set_title(title)\n",
    "        print(title)\n",
    "        print(disp.confusion_matrix)\n",
    "    plt.show()\n",
    "    print(\"********************************* Valores de precisión, recall y f-score *********************************\")\n",
    "    print(metrics.classification_report(y_test, regresion.predict(X_test_tratada)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite copiar ficheros entre directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copiaImagenes(src='ImagenesCovidExtra/', dst='Data/COVID-19/'):\n",
    "    if(not os.path.exists(src)):\n",
    "        print(\"¡Error! El directorio de origen no existe\")\n",
    "        return\n",
    "    elif(not os.path.exists(src)):\n",
    "        print(\"¡Error! El directorio de destino no existe\")\n",
    "        return\n",
    "    print(\"Copiando ficheros de \"+src+\" a \"+dst)\n",
    "    src_files = os.listdir(src)\n",
    "    for file_name in src_files:\n",
    "        full_file_name = os.path.join(src, file_name)\n",
    "        if os.path.isfile(full_file_name):\n",
    "            shutil.copy(full_file_name, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celda principal para la ejecución de las pruebas de la primera aproximación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Error! El path especificado no ha sido encontrado\n",
      "¡Error! El path especificado no ha sido encontrado\n",
      "¡Error! El path especificado no ha sido encontrado\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a4924c888e2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#Generamos el particionamiento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mlistaDatos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlistaCOVID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistaNORMAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistaVIRAL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparticionamiento\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistaDatos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#Almacenamos las imágenes en disco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-aaad5cbe47f8>\u001b[0m in \u001b[0;36mparticionamiento\u001b[1;34m(listaDatos, train_percent, validate_percent, seed)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlista\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistaDatos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mlista\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mperm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtrain_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_percent\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "#Constantes del problema:\n",
    "pathCovid = \"Data/COVID-19/\"\n",
    "pathNormal = \"Data/NORMAL/\"\n",
    "pathViral = \"Data/Viral Pneumonia/\" \n",
    "pathImagenesExtra = \"ImagenesCovidExtra/\"\n",
    "pathImagenes= \"Dataset/\"\n",
    "fichero_train = \"train.txt\"\n",
    "fichero_val = \"val.txt\"\n",
    "fichero_test = \"test.txt\"\n",
    "dx=256 #Tamaño de las imágenes en eje x\n",
    "dy=256 #Tamaño de las imágenes en eje y\n",
    "###############################################################################################################################\n",
    "\n",
    "#Pretratamiento de imagenes y almacenamiento de las mismas:\n",
    "#Realizamos el tratamiento de las imágenes:\n",
    "listaCOVID = tratamiento(pathCovid,dx,dy)\n",
    "listaNORMAL = tratamiento(pathNormal,dx,dy)\n",
    "listaVIRAL = tratamiento(pathViral,dx,dy)\n",
    "\n",
    "#Generamos el particionamiento\n",
    "listaDatos = [listaCOVID, listaNORMAL, listaVIRAL] \n",
    "train, test, val, y_train, y_test, y_val = particionamiento(listaDatos)\n",
    "\n",
    "#Almacenamos las imágenes en disco\n",
    "almacenaImagenes(train,test,val,y_train,y_test,y_val,pathImagenes)\n",
    "###############################################################################################################################\n",
    "\n",
    "#Lectura de las imágenes desde disco:\n",
    "train, y_train = lectura(\"train/\",fichero_train,pathImagenes)\n",
    "val, y_val = lectura(\"val/\",fichero_val,pathImagenes)\n",
    "test, y_test = lectura(\"test/\",fichero_test,pathImagenes)\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "#Dejamos las imágenes en 2D para la selección de características:\n",
    "Xtrain = np.reshape(train, (train.shape[0], dx*dy))\n",
    "Xtest = np.reshape(test, (test.shape[0], dx*dy))\n",
    "Xval = np.reshape(val, (val.shape[0], dx*dy))\n",
    "\n",
    "#Comprobación de las características y selección:\n",
    "Kbest = SelectKBest(f_classif) #Generamos el objeto SelectKBest\n",
    "Kbest.fit(Xtrain, y_train) #Entrenar el modelo\n",
    "numPixel = list(range(dx*dy)) #Generar el listado de número de píxeles\n",
    "\n",
    "#Generar un dataframe donde la primera columna sean el número de pixeles o características y la segundo el score:\n",
    "df = pd.DataFrame({'Pixeles': numPixel,'Score': Kbest.scores_.tolist()})\n",
    "#Generar un plot de barras para mostrar la información:\n",
    "plot = df.Score.plot(kind = 'hist', bins=40, title='Selección de Kbest')\n",
    "plot.set_xlabel(\"Score\")\n",
    "plot.set_ylabel(\"Frecuencia\")\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "##Pruebas##\n",
    "\n",
    "#Probamos a clasificar con todas las características y con lo parámetros por defecto del clasificador:\n",
    "print(\"********************************* Prueba clasificador con todas las características ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[0], norm=False)\n",
    "\n",
    "#Probamos a clasificar normalizando los datos de entrada:\n",
    "print(\"********************************* Prueba clasificador con todas las características normalizando ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[0], norm=True)\n",
    "\n",
    "#Probamos con diferentes valores de score:\n",
    "print(\"********************************* Prueba clasificador con varios valores de score ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[20,30,45,60], norm=False)\n",
    "\n",
    "#Probamos con diferentes valores de score y normalizando los datos:\n",
    "print(\"********************************* Prueba clasificador con varios valores de score normalizando ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[20,30,45,60], norm=True)\n",
    "\n",
    "#Probamos para un valor de score de 60 y generamos las matrices de confusión:\n",
    "print(\"********************************* Prueba clasificador con score 60 y normalizando ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[60], norm=True)\n",
    "genera_confusion(Xtrain,y_train)\n",
    "genera_confusion(Xval,y_val)\n",
    "genera_confusion(Xtest,y_test)\n",
    "\n",
    "#Hacemos un gridSearch para encontrar los mejores valores de los parámetros para la regresión logística:\n",
    "print(\"********************************* GridSearch para score 60 y diferentes algoritmos de optimización y normalización ********************************* \")\n",
    "pixelesSelec = df[df.Score >= 60]['Pixeles'].values.tolist()\n",
    "X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "params={'solver':['newton-cg','sag', 'saga', 'lbfgs'], 'penalty':['l1', 'l2', 'elasticnet'], 'C':[1,10,100,1000]}\n",
    "cv=5\n",
    "clf = GridSearchCV(LogisticRegression(), params, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "clf.fit(X_train_tratada,y_train)\n",
    "print(\"EL mejor resultado obtenido ha sido: \"+clf.best_score_+\", los mejores parámetros obtenidos son: \"+clf.best_params_)\n",
    "clasificador = LogisticRegression(solver='saga',penalty=\"l1\",C=1)\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[60], norm=True)\n",
    "\n",
    "#Hacemos otros gridSearch de nuevo pero para liblinear ya que hemos comprobado que es la mejor función de optimización para este problema:\n",
    "print(\"********************************* GridSearch para score 60 y diferentes algoritmos de normalización  ********************************* \")\n",
    "params={'solver':['liblinear'], 'penalty':['l1', 'l2', 'elasticnet'], 'C':[10,100,1000]}\n",
    "clf = GridSearchCV(LogisticRegression(), params, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "clf.fit(X_train_tratada,y_train)\n",
    "print(\"EL mejor resultado obtenido ha sido: \"+clf.best_score_+\", los mejores parámetros obtenidos son: \"+clf.best_params_)\n",
    "clasificador = LogisticRegression(solver='liblinear',penalty=\"l1\",C=100)\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[60], norm=True)\n",
    "\n",
    "#Añadimos nuevos ejemplos para COVID-19 y probamos con la mejor configuración obtenida:\n",
    "print(\"********************************* Prueba clasificador con más ejemplos en COVID-19 ********************************* \")\n",
    "copiaImagenes(pathImagenesExtra,pathCovid)\n",
    "clasificador = LogisticRegression(solver='liblinear',penalty=\"l1\",C=100)\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[60], norm=True)\n",
    "genera_confusion(Xtrain,y_train)\n",
    "genera_confusion(Xval,y_val)\n",
    "genera_confusion(Xtest,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda aproximación del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_caracteristicas_hog(X):\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    hog_descriptor = np.empty((n,2048))\n",
    "    for i in range(0,n):\n",
    "        fd = hog(X[i], orientations=8, pixels_per_cell=(16, 16),cells_per_block=(1, 1), visualize=False, multichannel=False)\n",
    "        hog_descriptor[i] = fd.reshape(1,-1)\n",
    "    return hog_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primera prueba de clasificación utilizando todas las características disponibles:\n",
    "pathCovid = \"Data/COVID-19/\"\n",
    "pathNormal = \"Data/NORMAL/\"\n",
    "pathViral = \"Data/Viral Pneumonia/\" \n",
    "pathImagenes= \"Dataset/\"\n",
    "fichero_train= \"train.txt\"\n",
    "fichero_test= \"test.txt\"\n",
    "fichero_val= \"val.txt\"\n",
    "dx=256 \n",
    "dy=256\n",
    "\n",
    "train, y_train = lectura(\"train/\",fichero_train,pathImagenes)\n",
    "val, y_val = lectura(\"val/\",fichero_val,pathImagenes)\n",
    "test, y_test = lectura(\"test/\",fichero_test,pathImagenes)\n",
    "\n",
    "X_train = obtener_caracteristicas_hog(train)\n",
    "X_val = obtener_caracteristicas_hog(val)\n",
    "X_test = obtener_caracteristicas_hog(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5228, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

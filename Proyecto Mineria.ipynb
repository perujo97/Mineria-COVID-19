{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROYECTO COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necesarios para el correcto funcionamiento del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest-shutil\n",
      "  Downloading https://files.pythonhosted.org/packages/26/b7/ef48a8f1f81ae4cd6f22992f6ffb7e9bf030d6e6654e2e626a05aaf5e880/pytest_shutil-1.7.0-py2.py3-none-any.whl\n",
      "Collecting execnet (from pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/2e/c63af07fa471e0a02d05793c7a56a9f7d274a8489442a5dc4fb3b2b3c705/execnet-1.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from pytest-shutil) (1.12.0)\n",
      "Collecting mock (from pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
      "Collecting path.py (from pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/0d/4caee829b04e3113b7069fa52063bce5c78e374e05850aa893549e917a1a/path.py-12.4.0-py3-none-any.whl\n",
      "Collecting termcolor (from pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting contextlib2 (from pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/85/60/370352f7ef6aa96c52fb001831622f50f923c1d575427d021b8ab3311236/contextlib2-0.6.0.post1-py2.py3-none-any.whl\n",
      "Collecting pytest (from pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/e2/c19c667f42f72716a7d03e8dd4d6f63f47d39feadd44cc1ee7ca3089862c/pytest-5.4.1-py3-none-any.whl (246kB)\n",
      "Collecting apipkg>=1.4 (from execnet->pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/67/08/4815a09603fc800209431bec5b8bd2acf2f95abdfb558a44a42507fb94da/apipkg-1.5-py2.py3-none-any.whl\n",
      "Collecting path<13.2 (from path.py->pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/4d/24/5827e075036b5bb6b538f71bf39574d4a8024c5df51206cb9d6739e24d94/path-13.1.0-py3-none-any.whl\n",
      "Collecting packaging (from pytest->pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/0a/34641d2bf5c917c96db0ded85ae4da25b6cd922d6b794648d4e7e07c88e5/packaging-20.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wcwidth in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from pytest->pytest-shutil) (0.1.7)\n",
      "Collecting importlib-metadata>=0.12; python_version < \"3.8\" (from pytest->pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/e4/891bfcaf868ccabc619942f27940c77a8a4b45fd8367098955bb7e152fb1/importlib_metadata-1.6.0-py2.py3-none-any.whl\n",
      "Collecting more-itertools>=4.0.0 (from pytest->pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/72/96/4297306cc270eef1e3461da034a3bebe7c84eff052326b130824e98fc3fb/more_itertools-8.2.0-py3-none-any.whl (43kB)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from pytest->pytest-shutil) (0.4.1)\n",
      "Collecting py>=1.5.0 (from pytest->pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/99/8d/21e1767c009211a62a8e3067280bfce76e89c9f876180308515942304d2d/py-1.8.1-py2.py3-none-any.whl (83kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from pytest->pytest-shutil) (19.1.0)\n",
      "Collecting atomicwrites>=1.0; sys_platform == \"win32\" (from pytest->pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/a0/da5f49008ec6e9a658dbf5d7310a4debd397bce0b4db03cf8a410066bb87/atomicwrites-1.4.0-py2.py3-none-any.whl\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest->pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from packaging->pytest->pytest-shutil) (2.4.2)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pytest-shutil)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4835 sha256=d3c95b6ea1685a5cc1a2b2b1b240117f46198fdf3ea5a1f75bff078b49c00852\n",
      "  Stored in directory: C:\\Users\\julen\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built termcolor\n",
      "Installing collected packages: apipkg, execnet, mock, zipp, importlib-metadata, path, path.py, termcolor, contextlib2, packaging, more-itertools, py, atomicwrites, pluggy, pytest, pytest-shutil\n",
      "Successfully installed apipkg-1.5 atomicwrites-1.4.0 contextlib2-0.6.0.post1 execnet-1.7.1 importlib-metadata-1.6.0 mock-4.0.2 more-itertools-8.2.0 packaging-20.3 path-13.1.0 path.py-12.4.0 pluggy-0.13.1 py-1.8.1 pytest-5.4.1 pytest-shutil-1.7.0 termcolor-1.1.0 zipp-3.1.0\n",
      "Requirement already satisfied: opencv-python in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (4.2.0.34)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from opencv-python) (1.16.4)\n",
      "Requirement already satisfied: imgaug in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from imgaug) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from imgaug) (1.16.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from imgaug) (4.2.0.34)\n",
      "Requirement already satisfied: Shapely in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from imgaug) (1.7.0)\n",
      "Requirement already satisfied: imageio in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from imgaug) (2.8.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from imgaug) (7.1.1)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from imgaug) (0.16.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from imgaug) (1.3.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from imgaug) (3.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (2.4)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (1.1.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from matplotlib->imgaug) (2.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from matplotlib->imgaug) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from matplotlib->imgaug) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from matplotlib->imgaug) (2.8.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug) (4.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\julen\\anaconda3\\envs\\py37machlearn\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest-shutil\n",
    "!pip install opencv-python\n",
    "!pip install imgaug\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from skimage.feature import local_binary_pattern\n",
    "%matplotlib inline\n",
    "import time\n",
    "from skimage.feature import hog\n",
    "import re\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primera aproximación del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite ordenar Strings alfanuméricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite la lectura de imágenes a partir del directorio en el que estan alamacenadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lectura(subdirectorio,fichero,path=\"Dataset/\"):\n",
    "    if(os.path.exists(path+subdirectorio)):\n",
    "        listaImagenes = sorted_alphanumeric(os.listdir(path+subdirectorio))\n",
    "        datos = []\n",
    "\n",
    "        print(\"Leyendo imágenes del directorio: \"+path+subdirectorio)\n",
    "        for imagen in listaImagenes:\n",
    "            datos.append(cv2.imread(path+subdirectorio+imagen,0))\n",
    "        if(os.path.isfile(path+fichero)):\n",
    "            with open(path+fichero) as fp: \n",
    "                salidas = fp.read().splitlines()\n",
    "            return np.array(datos), np.array(salidas)\n",
    "        else:\n",
    "            print(\"¡Error! El fichero de salidas no existe\")\n",
    "    else:\n",
    "        print(\"¡Error! El directorio no existe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la funcion para leer imagenes y aumentar el tamaño de nuestro conjunto de datos, utilizando:\n",
    "- Ruido Gaussiano\n",
    "- Volteando verticalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamiento(pathImagenes=\"Data/\", dx=256, dy=256, noiseX=5, noiseY=10):\n",
    "    if(os.path.exists(pathImagenes)):\n",
    "        print(\"Tratando y generando las imágenes: \"+pathImagenes)\n",
    "        resultado = []\n",
    "        listaImagenes = os.listdir(pathImagenes)\n",
    "        \n",
    "        #Realizar un data augmentation para aumentar los datos del dataset:\n",
    "        gaussian_noise = iaa.AdditiveGaussianNoise(noiseX, noiseY)\n",
    "        flip_vr=iaa.Fliplr(p=1.0)\n",
    "\n",
    "        for imagen in listaImagenes:\n",
    "            Xray = cv2.resize(cv2.imread(pathImagenes+imagen,0),(dx, dy))\n",
    "            resultado.append(Xray)\n",
    "            resultado.append(gaussian_noise.augment_image(Xray))\n",
    "            resultado.append(flip_vr.augment_image(Xray))\n",
    "\n",
    "        return resultado\n",
    "    else:\n",
    "        print(\"¡Error! El path especificado no ha sido encontrado\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una funcion para hacer el particionamiento de los datos en train, test y val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particionamiento(listaDatos, train_percent=.6, validate_percent=.2, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    listaTrain=[]\n",
    "    listaTest=[]\n",
    "    listaVal=[]\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    y_val = []\n",
    "    print(\"Generando el particionamiento...\")\n",
    "    for i,lista in enumerate(listaDatos):\n",
    "        lista = np.array(lista)\n",
    "        perm = np.random.permutation(len(lista))\n",
    "        m = len(lista)\n",
    "        train_end = int(train_percent * m)\n",
    "        val_end = int(validate_percent * m) + train_end\n",
    "        train = lista[perm[:train_end]]\n",
    "        val = lista[perm[train_end:val_end]]\n",
    "        test = lista[perm[val_end:]]\n",
    "        listaTrain.extend(train)\n",
    "        listaTest.extend(test)\n",
    "        listaVal.extend(val)\n",
    "        y_train.extend(list([i]*len(train)))\n",
    "        y_test.extend(list([i]*len(test)))\n",
    "        y_val.extend(list([i]*len(val)))\n",
    "    \n",
    "    return listaTrain, listaTest, listaVal, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función permite almacenar las imágenes leidas y tratasdas durante la fase de train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def almacenaImagenes(train,test,val,y_train,y_test,y_val,path=\"Dataset/\"):\n",
    "    if(os.path.exists(path)):\n",
    "        try:\n",
    "            shutil.rmtree(path,ignore_errors=True)\n",
    "            \n",
    "        except OSError as e:\n",
    "            print(\"¡Error! No se ha podido eliminar el directorio\")\n",
    "    print(\"Almacenando las imágenes en disco...\")\n",
    "    os.mkdir(path, 0o7777)\n",
    "    os.mkdir(path+\"train/\",0o7777)\n",
    "    os.mkdir(path+\"val/\",0o7777)\n",
    "    os.mkdir(path+\"test/\",0o7777)\n",
    "    \n",
    "    f = open(path+\"train.txt\", \"w\")\n",
    "    for i,imagen in enumerate(train):\n",
    "        cv2.imwrite(path+\"train/Train-\"+str(i)+\".png\",imagen)\n",
    "        f.write(str(y_train[i])+\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    f = open(path+\"test.txt\", \"w\")\n",
    "    for i,imagen in enumerate(test):\n",
    "        cv2.imwrite(path+\"test/Test-\"+str(i)+\".png\",imagen)\n",
    "        f.write(str(y_test[i])+\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    f = open(path+\"val.txt\", \"w\")\n",
    "    for i,imagen in enumerate(val):\n",
    "        cv2.imwrite(path+\"val/Val-\"+str(i)+\".png\",imagen)\n",
    "        f.write(str(y_val[i])+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite el entrenamiento y la clasificación utilizando un clasificador que se pasa como parámetro, además los datos pueden ser normalizados si el parámetro norm tiene el valor True. También permite utilizar diferentes valores de score con los que seleccionar el conjunto de características de los datos que se pasan como parámetros. Devuelve el tiempo de ejecución así como los valores de accuracy obtenidos en train, validación y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[0], norm=False):\n",
    "\n",
    "    #Normalización de los datos:\n",
    "    if(norm == True):\n",
    "        Xtrain = Xtrain/255\n",
    "        Xtest = Xtest/255\n",
    "        Xval = Xval/255\n",
    "\n",
    "    accTrain=[]\n",
    "    accTest=[]\n",
    "    accVal=[]\n",
    "    #Comenzar a contar el tiempo de ejecución:\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i,k in enumerate(score):\n",
    "        #Seleccionar aquellos pixeles que tengan un Score mayor o igual al establecido\n",
    "        pixelesSelec = df[df.Score >= k]['Pixeles'].values.tolist()\n",
    "\n",
    "        #Entrenar el modelo con los ejemplos con el nuevo numero de caracteristicas:\n",
    "        #Generar el nuevo conjunto de Train:\n",
    "        X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "\n",
    "        #Entrenar el modelo:\n",
    "        clasificador.fit(X_train_tratada,y_train)\n",
    "\n",
    "        #Obtener la predcción en train y el accuracy:\n",
    "        predictTrain = clasificador.predict(X_train_tratada)\n",
    "        accTrain.append(metrics.accuracy_score(predictTrain,y_train)*100)\n",
    "\n",
    "        #Generar el nuevo conjunto de Val:\n",
    "        X_val_tratada = Xval[:,pixelesSelec]\n",
    "\n",
    "        #Obtener la predcción en val y el accuracy:\n",
    "        predictVal = clasificador.predict(X_val_tratada)\n",
    "        accVal.append(metrics.accuracy_score(predictVal,y_val)*100)\n",
    "\n",
    "        #Generar el nuevo conjunto de Test:\n",
    "        X_test_tratada = Xtest[:,pixelesSelec]\n",
    "\n",
    "        #Obtener la predcción en test y el accuracy:\n",
    "        predictTest = clasificador.predict(X_test_tratada)\n",
    "        accTest.append(metrics.accuracy_score(predictTest,y_test)*100)\n",
    "\n",
    "        print('El rendimiento en entrenamiento con {} variables para un score de {} es de {}%'.format(len(pixelesSelec),k,accTrain[i]))\n",
    "        print('El rendimiento en validacion con {} variables para un score de {}  es de {}%'.format(len(pixelesSelec),k,accVal[i]))\n",
    "        print('El rendimiento en test con {} variables para un score de {}  es de {}%'.format(len(pixelesSelec),k,accTest[i]))\n",
    "\n",
    "    tiempo_ejecucion = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (tiempo_ejecucion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función genera la matriz de confusión para un conjunto de datos de entrada y muestra los valores de precisión, recall y f-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_confusion(clasificador,X,y):\n",
    "    print(\"********************************* Plot de matriz de confusión *********************************\")\n",
    "    class_names = ['COVID-19','Normal','Viral']\n",
    "    titles_options = [(\"Matriz de confusion sin normalizar\", None),\n",
    "                      (\"Matriz de confusion normalizada\", 'true')]\n",
    "    for title, normalize in titles_options:\n",
    "        disp = metrics.plot_confusion_matrix(clasificador, X, y,\n",
    "                                     display_labels=class_names,\n",
    "                                     cmap=plt.cm.Blues,\n",
    "                                     normalize=normalize)\n",
    "        disp.ax_.set_title(title)\n",
    "        print(title)\n",
    "        print(disp.confusion_matrix)\n",
    "    plt.show()\n",
    "    print(\"********************************* Valores de precisión, recall y f-score *********************************\")\n",
    "    print(metrics.classification_report(y, clasificador.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite copiar ficheros entre directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copiaImagenes(src='ImagenesCovidExtra/', dst='Data/COVID-19/'):\n",
    "    if(not os.path.exists(src)):\n",
    "        print(\"¡Error! El directorio de origen no existe\")\n",
    "        return\n",
    "    elif(not os.path.exists(src)):\n",
    "        print(\"¡Error! El directorio de destino no existe\")\n",
    "        return\n",
    "    print(\"Copiando ficheros de \"+src+\" a \"+dst)\n",
    "    src_files = os.listdir(src)\n",
    "    for file_name in src_files:\n",
    "        full_file_name = os.path.join(src, file_name)\n",
    "        if os.path.isfile(full_file_name):\n",
    "            shutil.copy(full_file_name, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite eliminar las imágenes extra de COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminaImagenesExtra(src='ImagenesCovidExtra/', dst='Data/COVID-19/'):\n",
    "    if(os.path.exists(src)):\n",
    "        if(os.path.exists(dst)):\n",
    "            print(\"Eliminando las imágenes extra de: \"+dst)\n",
    "            \n",
    "            listaImagenes = os.listdir(src)\n",
    "            for imagen in listaImagenes:\n",
    "                fichero = dst+imagen\n",
    "                if os.path.exists(fichero):\n",
    "                    os.remove(fichero)\n",
    "        else:\n",
    "            print(\"Path de destino no encontrado \"+dst)\n",
    "    else:\n",
    "         print(\"Path de origen no encontrado \"+src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celda principal para la ejecución de las pruebas de la primera aproximación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Constantes del problema:\n",
    "pathCovid = \"Data/COVID-19/\"\n",
    "pathNormal = \"Data/NORMAL/\"\n",
    "pathViral = \"Data/Viral Pneumonia/\" \n",
    "pathImagenesExtra = \"ImagenesCovidExtra/\"\n",
    "pathImagenes= \"Dataset/\"\n",
    "fichero_train = \"train.txt\"\n",
    "fichero_val = \"val.txt\"\n",
    "fichero_test = \"test.txt\"\n",
    "dx=256 #Tamaño de las imágenes en eje x\n",
    "dy=256 #Tamaño de las imágenes en eje y\n",
    "np.random.seed(0)\n",
    "###############################################################################################################################\n",
    "\n",
    "#Pretratamiento de imagenes y almacenamiento de las mismas:\n",
    "#Realizamos el tratamiento de las imágenes:\n",
    "listaCOVID = tratamiento(pathCovid,dx,dy)\n",
    "listaNORMAL = tratamiento(pathNormal,dx,dy)\n",
    "listaVIRAL = tratamiento(pathViral,dx,dy)\n",
    "\n",
    "#Generamos el particionamiento\n",
    "listaDatos = [listaCOVID, listaNORMAL, listaVIRAL] \n",
    "train, test, val, y_train, y_test, y_val = particionamiento(listaDatos)\n",
    "\n",
    "#Almacenamos las imágenes en disco\n",
    "almacenaImagenes(train,test,val,y_train,y_test,y_val,pathImagenes)\n",
    "###############################################################################################################################\n",
    "\n",
    "#Lectura de las imágenes desde disco:\n",
    "train, y_train = lectura(\"train/\",fichero_train,pathImagenes)\n",
    "val, y_val = lectura(\"val/\",fichero_val,pathImagenes)\n",
    "test, y_test = lectura(\"test/\",fichero_test,pathImagenes)\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "#Dejamos las imágenes en 2D para la selección de características:\n",
    "Xtrain = np.reshape(train, (train.shape[0], dx*dy))\n",
    "Xtest = np.reshape(test, (test.shape[0], dx*dy))\n",
    "Xval = np.reshape(val, (val.shape[0], dx*dy))\n",
    "\n",
    "#Comprobación de las características y selección:\n",
    "Kbest = SelectKBest(f_classif) #Generamos el objeto SelectKBest\n",
    "Kbest.fit(Xtrain, y_train) #Entrenar el modelo\n",
    "numPixel = list(range(dx*dy)) #Generar el listado de número de píxeles\n",
    "\n",
    "#Generar un dataframe donde la primera columna sean el número de pixeles o características y la segundo el score:\n",
    "df = pd.DataFrame({'Pixeles': numPixel,'Score': Kbest.scores_.tolist()})\n",
    "#Generar un plot de barras para mostrar la información:\n",
    "plot = df.Score.plot(kind = 'hist', bins=40, title='Selección de Kbest')\n",
    "plot.set_xlabel(\"Score\")\n",
    "plot.set_ylabel(\"Frecuencia\")\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "############################################ Pruebas con Regresión logística ##################################################\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################\n",
    "\n",
    "#Probamos a clasificar con todas las características y con lo parámetros por defecto del clasificador:\n",
    "print(\"********************************* Prueba clasificador con todas las características ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[0], norm=False)\n",
    "\n",
    "#Probamos a clasificar normalizando los datos de entrada:\n",
    "print(\"********************************* Prueba clasificador con todas las características normalizando ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[0], norm=True)\n",
    "\n",
    "#Probamos con diferentes valores de score:\n",
    "print(\"********************************* Prueba clasificador con varios valores de score ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[100,200,300,450], norm=False)\n",
    "\n",
    "#Probamos con diferentes valores de score y normalizando los datos:\n",
    "print(\"********************************* Prueba clasificador con varios valores de score normalizando ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[100,200,300,450], norm=True)\n",
    "\n",
    "#Probamos a calcular los tiempos de ejecución con score 100 normalizando los datos de entrada:\n",
    "print(\"********************************* Prueba clasificador tiempo ejecución con score 100 normalizando ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[100], norm=True)\n",
    "\n",
    "#Probamos a calcular los tiempos de ejecución con score 200 normalizando los datos de entrada:\n",
    "print(\"********************************* Prueba clasificador tiempo ejecución con score 200 normalizandoo ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[200], norm=True)\n",
    "\n",
    "# #Probamos a calcular los tiempos de ejecución con score 300 normalizando los datos de entrada:\n",
    "print(\"********************************* Prueba clasificador tiempo ejecución con score 300 normalizando ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[300], norm=True)\n",
    "\n",
    "# #Probamos a calcular los tiempos de ejecución con score 450 normalizando los datos de entrada:\n",
    "print(\"********************************* Prueba clasificador tiempo ejecución con score 450 normalizando ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[450], norm=True)\n",
    "\n",
    "#Probamos para un valor de score de 200 y generamos las matrices de confusión:\n",
    "print(\"********************************* Prueba clasificador con score 200 y normalizando ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[200], norm=True)\n",
    "pixelesSelec = df[df.Score >= 200]['Pixeles'].values.tolist()\n",
    "X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "X_val_tratada = Xval[:,pixelesSelec]\n",
    "X_test_tratada = Xtest[:,pixelesSelec]\n",
    "genera_confusion(clasificador,X_train_tratada,y_train)\n",
    "genera_confusion(clasificador,X_val_tratada,y_val)\n",
    "genera_confusion(clasificador,X_test_tratada,y_test)\n",
    "\n",
    "#Hacemos un gridSearch para encontrar los mejores valores de los parámetros para la regresión logística:\n",
    "print(\"********************************* GridSearch para score 200 y diferentes algoritmos de optimización y normalización ********************************* \")\n",
    "params={'solver':['newton-cg','sag', 'saga', 'lbfgs'], 'penalty':['l1', 'l2', 'elasticnet'], 'C':[1,10,100,1000]}\n",
    "cv=5\n",
    "clf = GridSearchCV(LogisticRegression(), params, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "pixelesSelec = df[df.Score >= 200]['Pixeles'].values.tolist()\n",
    "X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "clf.fit(X_train_tratada,y_train)\n",
    "print(\"EL mejor resultado obtenido ha sido: \"+str(clf.best_score_)+\", los mejores parámetros obtenidos son: \"+str(clf.best_params_))\n",
    "clasificador = LogisticRegression(solver='saga',penalty=\"l2\",C=100,n_jobs=-1)\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[200], norm=True)\n",
    "pixelesSelec = df[df.Score >= 200]['Pixeles'].values.tolist()\n",
    "X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "X_val_tratada = Xval[:,pixelesSelec]\n",
    "X_test_tratada = Xtest[:,pixelesSelec]\n",
    "genera_confusion(clasificador,X_train_tratada,y_train)\n",
    "genera_confusion(clasificador,X_val_tratada,y_val)\n",
    "genera_confusion(clasificador,X_test_tratada,y_test)\n",
    "\n",
    "#Hacemos otros gridSearch de nuevo pero para liblinear ya que hemos comprobado que es la mejor función de optimización para este problema:\n",
    "print(\"********************************* GridSearch para score 200 y diferentes algoritmos de normalización  ********************************* \")\n",
    "params={'solver':['liblinear'], 'penalty':['l1', 'l2', 'elasticnet'], 'C':[2,5,10,100]}\n",
    "clf = GridSearchCV(LogisticRegression(), params, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "clf.fit(X_train_tratada,y_train)\n",
    "print(\"EL mejor resultado obtenido ha sido: \"+str(clf.best_score_)+\", los mejores parámetros obtenidos son: \"+str(clf.best_params_))\n",
    "clasificador = LogisticRegression(solver='liblinear',penalty=\"l1\",C=5)\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[200], norm=True)\n",
    "pixelesSelec = df[df.Score >= 200]['Pixeles'].values.tolist()\n",
    "X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "X_val_tratada = Xval[:,pixelesSelec]\n",
    "X_test_tratada = Xtest[:,pixelesSelec]\n",
    "genera_confusion(clasificador,X_train_tratada,y_train)\n",
    "genera_confusion(clasificador,X_val_tratada,y_val)\n",
    "genera_confusion(clasificador,X_test_tratada,y_test)\n",
    "\n",
    "#Añadimos nuevos ejemplos para COVID-19 y probamos con la mejor configuración obtenida:\n",
    "print(\"********************************* Prueba clasificador con más ejemplos de COVID-19 ********************************* \")\n",
    "#Copiar las nuevas imagenes al directorio inicial de covid para tratarlas:\n",
    "copiaImagenes(pathImagenesExtra,pathCovid)\n",
    "#Pretratamiento de imagenes y almacenamiento de las mismas:\n",
    "#Realizamos el tratamiento de las imágenes:\n",
    "listaCOVID = tratamiento(pathCovid,dx,dy)\n",
    "listaNORMAL = tratamiento(pathNormal,dx,dy)\n",
    "listaVIRAL = tratamiento(pathViral,dx,dy)\n",
    "\n",
    "#Generamos el particionamiento\n",
    "listaDatos = [listaCOVID, listaNORMAL, listaVIRAL] \n",
    "train, test, val, y_train, y_test, y_val = particionamiento(listaDatos)\n",
    "\n",
    "#Almacenamos las imágenes en disco\n",
    "almacenaImagenes(train,test,val,y_train,y_test,y_val,pathImagenes)\n",
    "###############################################################################################################################\n",
    "\n",
    "#Lectura de las imágenes desde disco:\n",
    "train, y_train = lectura(\"train/\",fichero_train,pathImagenes)\n",
    "val, y_val = lectura(\"val/\",fichero_val,pathImagenes)\n",
    "test, y_test = lectura(\"test/\",fichero_test,pathImagenes)\n",
    "\n",
    "Xtrain = np.reshape(train, (train.shape[0], dx*dy))\n",
    "Xtest = np.reshape(test, (test.shape[0], dx*dy))\n",
    "Xval = np.reshape(val, (val.shape[0], dx*dy))\n",
    "\n",
    "#Comprobación de las características y selección:\n",
    "Kbest = SelectKBest(f_classif) #Generamos el objeto SelectKBest\n",
    "Kbest.fit(Xtrain, y_train) #Entrenar el modelo\n",
    "numPixel = list(range(dx*dy)) #Generar el listado de número de píxeles\n",
    "\n",
    "#Generar un dataframe donde la primera columna sean el número de pixeles o características y la segundo el score:\n",
    "df = pd.DataFrame({'Pixeles': numPixel,'Score': Kbest.scores_.tolist()})\n",
    "\n",
    "###############################################################################################################################\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[200], norm=True)\n",
    "pixelesSelec = df[df.Score >= 200]['Pixeles'].values.tolist()\n",
    "X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "X_val_tratada = Xval[:,pixelesSelec]\n",
    "X_test_tratada = Xtest[:,pixelesSelec]\n",
    "genera_confusion(clasificador,X_train_tratada,y_train)\n",
    "genera_confusion(clasificador,X_val_tratada,y_val)\n",
    "genera_confusion(clasificador,X_test_tratada,y_test)\n",
    "\n",
    "eliminaImagenesExtra(pathImagenesExtra,pathCovid)\n",
    "############################################ Fin de pruebas con Regresión logística ###########################################\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################\n",
    "\n",
    "############################################ Pruebas con Support Vector Machines ##############################################\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################\n",
    "\n",
    "#Pretratamiento de imagenes y almacenamiento de las mismas:\n",
    "#Realizamos el tratamiento de las imágenes:\n",
    "listaCOVID = tratamiento(pathCovid,dx,dy)\n",
    "listaNORMAL = tratamiento(pathNormal,dx,dy)\n",
    "listaVIRAL = tratamiento(pathViral,dx,dy)\n",
    "\n",
    "#Generamos el particionamiento\n",
    "listaDatos = [listaCOVID, listaNORMAL, listaVIRAL] \n",
    "train, test, val, y_train, y_test, y_val = particionamiento(listaDatos)\n",
    "\n",
    "#Almacenamos las imágenes en disco\n",
    "almacenaImagenes(train,test,val,y_train,y_test,y_val,pathImagenes)\n",
    "###############################################################################################################################\n",
    "\n",
    "#Lectura de las imágenes desde disco:\n",
    "train, y_train = lectura(\"train/\",fichero_train,pathImagenes)\n",
    "val, y_val = lectura(\"val/\",fichero_val,pathImagenes)\n",
    "test, y_test = lectura(\"test/\",fichero_test,pathImagenes)\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "#Dejamos las imágenes en 2D para la selección de características:\n",
    "Xtrain = np.reshape(train, (train.shape[0], dx*dy))\n",
    "Xtest = np.reshape(test, (test.shape[0], dx*dy))\n",
    "Xval = np.reshape(val, (val.shape[0], dx*dy))\n",
    "\n",
    "#Comprobación de las características y selección:\n",
    "Kbest = SelectKBest(f_classif) #Generamos el objeto SelectKBest\n",
    "Kbest.fit(Xtrain, y_train) #Entrenar el modelo\n",
    "numPixel = list(range(dx*dy)) #Generar el listado de número de píxeles\n",
    "\n",
    "#Generar un dataframe donde la primera columna sean el número de pixeles o características y la segundo el score:\n",
    "df = pd.DataFrame({'Pixeles': numPixel,'Score': Kbest.scores_.tolist()})\n",
    "\n",
    "#Hacemos un gridSearch para encontrar los mejores valores de los parámetros para la SVC\n",
    "print(\"********************************* GridSearch para score 100 y SVC y normalización ********************************* \")\n",
    "pixelesSelec = df[df.Score >= 100]['Pixeles'].values.tolist()\n",
    "X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "gs_clf_svm = model_selection.GridSearchCV(estimator=svm.SVC(), param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')}\n",
    "                                          ,return_train_score=True, n_jobs = -1)\n",
    "gs_clf_svm.fit(X_train_tratada,y_train)\n",
    "print(\"EL mejor resultado obtenido ha sido: \"+str(gs_clf_svm.best_score_)+\", los mejores parámetros obtenidos son: \"+str(gs_clf_svm.best_params_))\n",
    "clasificador = svm.SVC(gs_clf_svm.best_score_)\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[100], norm=True)\n",
    "\n",
    "#Adaboost para LR\n",
    "print(\"********************************* Prueba clasificador Adaboost para LR ********************************* \")\n",
    "LR = LogisticRegression(solver='liblinear')\n",
    "adaBoostLR = AdaBoostClassifier(base_estimator=LR, n_estimators= 4)\n",
    "clasifica(adaBoostLR, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[100], norm=True)\n",
    "\n",
    "#Adaboost para Tree\n",
    "print(\"********************************* Prueba clasificador Adaboost para Tree ********************************* \")\n",
    "adaBoost = AdaBoostClassifier(n_estimators= 100)\n",
    "clasifica(adaBoost, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[100], norm=True)\n",
    "\n",
    "#OVR\n",
    "print(\"********************************* Prueba clasificador OVR ********************************* \")\n",
    "LogReg = LogisticRegression(solver='liblinear', C=10, penalty='l1')\n",
    "OVR = OneVsRestClassifier(LogReg).fit(X_train_tratada,y_train)\n",
    "clasifica(OVR, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[100], norm=True)\n",
    "\n",
    "#Hacemos un gridSearch para encontrar los mejores valores de los parámetros para la SVC\n",
    "print(\"********************************* GridSearch para score 200 y SVC y normalización ********************************* \")\n",
    "pixelesSelec = df[df.Score >= 200]['Pixeles'].values.tolist()\n",
    "X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "X_test_tratada = Xtest[:,pixelesSelec]\n",
    "X_val_tratada = Xval[:,pixelesSelec]\n",
    "clasificador = svm.SVC(C=10, kernel = 'rbf')\n",
    "clasifica(clasificador, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[200], norm=True)\n",
    "genera_confusion(clasificador,X_train_tratada,y_train)\n",
    "genera_confusion(clasificador,X_val_tratada,y_val)\n",
    "genera_confusion(clasificador,X_test_tratada,y_test)\n",
    "\n",
    "#Adaboost para LR\n",
    "print(\"********************************* Prueba clasificador Adaboost para LR ********************************* \")\n",
    "LR = LogisticRegression(solver='liblinear')\n",
    "adaBoostLR = AdaBoostClassifier(base_estimator=LR, n_estimators= 4)\n",
    "clasifica(adaBoostLR, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[200], norm=True)\n",
    "genera_confusion(adaBoostLR,X_train_tratada,y_train)\n",
    "genera_confusion(adaBoostLR,X_val_tratada,y_val)\n",
    "genera_confusion(adaBoostLR,X_test_tratada,y_test)\n",
    "\n",
    "#Adaboost para Tree\n",
    "print(\"********************************* Prueba clasificador Adaboost para Tree ********************************* \")\n",
    "adaBoost = AdaBoostClassifier(n_estimators= 100)\n",
    "clasifica(adaBoost, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[200], norm=True)\n",
    "genera_confusion(adaBoost,X_train_tratada,y_train)\n",
    "genera_confusion(adaBoost,X_val_tratada,y_val)\n",
    "genera_confusion(adaBoost,X_test_tratada,y_test)\n",
    "\n",
    "#OVR\n",
    "print(\"********************************* Prueba clasificador OVR ********************************* \")\n",
    "LogReg = LogisticRegression(solver='liblinear', C=10, penalty='l1')\n",
    "OVR = OneVsRestClassifier(LogReg).fit(X_train_tratada,y_train)\n",
    "clasifica(OVR, df, Xtrain, Xtest, Xval, y_train, y_test, y_val, score=[200], norm=True)\n",
    "genera_confusion(OVR,X_train_tratada,y_train)\n",
    "genera_confusion(OVR,X_val_tratada,y_val)\n",
    "genera_confusion(OVR,X_test_tratada,y_test)\n",
    "\n",
    "############################################ Fin de pruebas con SVM ###########################################################\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda aproximación del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite obtener las características de HOG para cada imagen del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_caracteristicas_hog(X,numFeatures):\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    hog_descriptor = np.empty((n,numFeatures))\n",
    "    for i in range(0,n):\n",
    "        fd = hog(X[i], orientations=8, pixels_per_cell=(16, 16),cells_per_block=(1, 1), visualize=False, multichannel=False)\n",
    "        hog_descriptor[i] = fd.reshape(1,-1)\n",
    "    return hog_descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite el entrenamiento y la clasificación utilizando un clasificador que se pasa como parámetro, además los datos pueden ser normalizados si el parámetro norm tiene el valor True. Devuelve el tiempo de ejecución así como los valores de accuracy obtenidos en train, validación y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasifica_segunda(clasificador, Xtrain, Xtest, Xval, y_train, y_test, y_val):\n",
    "    \n",
    "    accTrain=[]\n",
    "    accTest=[]\n",
    "    accVal=[]\n",
    "    start_time = time.time()\n",
    "\n",
    "    #Entrenar el modelo:\n",
    "    clasificador.fit(Xtrain,y_train)\n",
    "\n",
    "    #Obtener la predcción en train y el accuracy:\n",
    "    predictTrain = clasificador.predict(Xtrain)\n",
    "    accTrain.append(metrics.accuracy_score(predictTrain,y_train)*100)\n",
    "\n",
    "    #Obtener la predcción en val y el accuracy:\n",
    "    predictVal = clasificador.predict(Xval)\n",
    "    accVal.append(metrics.accuracy_score(predictVal,y_val)*100)\n",
    "\n",
    "    #Obtener la predcción en test y el accuracy:\n",
    "    predictTest = clasificador.predict(Xtest)\n",
    "    accTest.append(metrics.accuracy_score(predictTest,y_test)*100)\n",
    "\n",
    "    print('El rendimiento en entrenamiento es de {}%'.format(accTrain[0]))\n",
    "    print('El rendimiento en validacion es de {}%'.format(accVal[0]))\n",
    "    print('El rendimiento en test es de {}%'.format(accTest[0]))\n",
    "\n",
    "    tiempo_ejecucion = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (tiempo_ejecucion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celda principal para la ejecución de las pruebas de la segunda aproximación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constantes del problema \n",
    "pathCovid = \"Data/COVID-19/\"\n",
    "pathNormal = \"Data/NORMAL/\"\n",
    "pathViral = \"Data/Viral Pneumonia/\" \n",
    "pathImagenesExtra = \"ImagenesCovidExtra/\"\n",
    "pathImagenes= \"Dataset/\"\n",
    "fichero_train = \"train.txt\"\n",
    "fichero_val = \"val.txt\"\n",
    "fichero_test = \"test.txt\"\n",
    "dx=256 #Tamaño de las imágenes en eje x\n",
    "dy=256 #Tamaño de las imágenes en eje y\n",
    "numFeatures = 2048\n",
    "METHOD = 'uniform'\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "c = [2,5,10,100]\n",
    "np.random.seed(0)\n",
    "\n",
    "train, y_train = lectura(\"train/\",fichero_train,pathImagenes)\n",
    "val, y_val = lectura(\"val/\",fichero_val,pathImagenes)\n",
    "test, y_test = lectura(\"test/\",fichero_test,pathImagenes)\n",
    "\n",
    "#Extraer Las características con HOG:\n",
    "X_train = obtener_caracteristicas_hog(train)\n",
    "X_val = obtener_caracteristicas_hog(val)\n",
    "X_test = obtener_caracteristicas_hog(test)\n",
    "\n",
    "############################################ Pruebas con HOG ##################################################################\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################\n",
    "\n",
    "# Probamos a clasificar con todas las características y con lo parámetros por defecto del clasificador:\n",
    "print(\"********************************* Prueba clasificador con todas las características de HOG ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica_segunda(clasificador, X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "genera_confusion(clasificador,X_train,y_train)\n",
    "genera_confusion(clasificador,X_test,y_test)\n",
    "genera_confusion(clasificador,X_val,y_val)\n",
    "\n",
    "# Añadimos nuevos ejemplos para COVID-19 y probamos con la mejor configuración obtenida:\n",
    "#Copiar las nuevas imagenes al directorio inicial de covid para tratarlas:\n",
    "copiaImagenes(pathImagenesExtra,pathCovid)\n",
    "#Pretratamiento de imagenes y almacenamiento de las mismas:\n",
    "#Realizamos el tratamiento de las imágenes:\n",
    "listaCOVID = tratamiento(pathCovid,dx,dy)\n",
    "listaNORMAL = tratamiento(pathNormal,dx,dy)\n",
    "listaVIRAL = tratamiento(pathViral,dx,dy)\n",
    "\n",
    "#Generamos el particionamiento\n",
    "listaDatos = [listaCOVID, listaNORMAL, listaVIRAL] \n",
    "train, test, val, y_train, y_test, y_val = particionamiento(listaDatos)\n",
    "\n",
    "#Almacenamos las imágenes en disco\n",
    "almacenaImagenes(train,test,val,y_train,y_test,y_val,pathImagenes)\n",
    "###############################################################################################################################\n",
    "\n",
    "#Lectura de las imágenes desde disco:\n",
    "train, y_train = lectura(\"train/\",fichero_train,pathImagenes)\n",
    "val, y_val = lectura(\"val/\",fichero_val,pathImagenes)\n",
    "test, y_test = lectura(\"test/\",fichero_test,pathImagenes)\n",
    "###############################################################################################################################\n",
    "\n",
    "#Extraer Las características con HOG:\n",
    "X_train = obtener_caracteristicas_hog(train,numFeatures)\n",
    "X_val = obtener_caracteristicas_hog(val,numFeatures)\n",
    "X_test = obtener_caracteristicas_hog(test,numFeatures)\n",
    "\n",
    "print(\"********************************* Prueba clasificador con más ejemplos de COVID-19 ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear')\n",
    "clasifica_segunda(clasificador, X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "genera_confusion(clasificador,X_train,y_train)\n",
    "genera_confusion(clasificador,X_test,y_test)\n",
    "genera_confusion(clasificador,X_val,y_val)\n",
    "\n",
    "print(\"********************************* Prueba clasificador con regularización para más ejemplos de COVID-19 ********************************* \")\n",
    "clasificador = LogisticRegression(solver='liblinear',C=5,penalty=\"l1\")\n",
    "clasifica_segunda(clasificador, X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "genera_confusion(clasificador,X_train,y_train)\n",
    "genera_confusion(clasificador,X_test,y_test)\n",
    "genera_confusion(clasificador,X_val,y_val)\n",
    "\n",
    "#Probamos a utilizar kbest de nuevo para ver si mejoramos los resultados que hemos obtenido:\n",
    "print(\"********************************* Prueba clasificador con menor número de características ********************************* \")\n",
    "#Comprobación de las características y selección:\n",
    "Kbest = SelectKBest(f_classif) #Generamos el objeto SelectKBest\n",
    "Kbest.fit(X_train, y_train) #Entrenar el modelo\n",
    "numPixel = list(range(numFeatures)) #Generar el listado de número de píxeles\n",
    "\n",
    "#Generar un dataframe donde la primera columna sean el número de características y la segundo el score:\n",
    "df = pd.DataFrame({'Pixeles': numPixel,'Score': Kbest.scores_.tolist()})\n",
    "#Generar un plot de barras para mostrar la información:\n",
    "plot = df.Score.plot(kind = 'hist', bins=40, title='Selección de Kbest')\n",
    "plot.set_xlabel(\"Score\")\n",
    "plot.set_ylabel(\"Frecuencia\")\n",
    "scores=[25,50,75,100,150,200]\n",
    "for score in scores:\n",
    "    clasificador = LogisticRegression(solver='liblinear')\n",
    "    clasifica(clasificador, df, X_train, X_test, X_val, y_train, y_test, y_val, score=[score], norm=False)\n",
    "    pixelesSelec = df[df.Score >= score]['Pixeles'].values.tolist()\n",
    "    X_train_tratada = X_train[:,pixelesSelec]\n",
    "    X_val_tratada = X_val[:,pixelesSelec]\n",
    "    X_test_tratada = X_test[:,pixelesSelec]\n",
    "    genera_confusion(clasificador,X_train_tratada,y_train)\n",
    "    genera_confusion(clasificador,X_val_tratada,y_val)\n",
    "    genera_confusion(clasificador,X_test_tratada,y_test)\n",
    "\n",
    "#Probamos a utilizar kbest de nuevo para ver si mejoramos los resultados que hemos obtenido:\n",
    "print(\"********************************* Prueba clasificador OVR con HOG y más ejemplos COVID-19 ********************************* \")\n",
    "LogReg = LogisticRegression(solver='liblinear')\n",
    "OVR = OneVsRestClassifier(LogReg).fit(X_train,y_train)\n",
    "clasifica_segunda(OVR, X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "genera_confusion(OVR,X_train,y_train)\n",
    "genera_confusion(OVR,X_val,y_val)\n",
    "genera_confusion(OVR,X_test,y_test)\n",
    "eliminaImagenesExtra(pathImagenesExtra,pathCovid)\n",
    "\n",
    "############################################ Fin de pruebas con HOG ###########################################################\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################\n",
    "\n",
    "############################################ Pruebas con LBP ##################################################################\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################\n",
    "\n",
    "#Pretratamiento de imagenes y almacenamiento de las mismas:\n",
    "#Realizamos el tratamiento de las imágenes:\n",
    "listaCOVID = tratamiento(pathCovid,dx,dy)\n",
    "listaNORMAL = tratamiento(pathNormal,dx,dy)\n",
    "listaVIRAL = tratamiento(pathViral,dx,dy)\n",
    "\n",
    "#Generamos el particionamiento\n",
    "listaDatos = [listaCOVID, listaNORMAL, listaVIRAL] \n",
    "train, test, val, y_train, y_test, y_val = particionamiento(listaDatos)\n",
    "\n",
    "#Almacenamos las imágenes en disco\n",
    "almacenaImagenes(train,test,val,y_train,y_test,y_val,pathImagenes)\n",
    "###############################################################################################################################\n",
    "\n",
    "#Lectura de las imágenes desde disco:\n",
    "train, y_train = lectura(\"train/\",fichero_train,pathImagenes)\n",
    "val, y_val = lectura(\"val/\",fichero_val,pathImagenes)\n",
    "test, y_test = lectura(\"test/\",fichero_test,pathImagenes)\n",
    "\n",
    "#Comprobación de las características y selección:\n",
    "Kbest = SelectKBest(f_classif) #Generamos el objeto SelectKBest\n",
    "Kbest.fit(Xtrain, y_train) #Entrenar el modelo\n",
    "numPixel = list(range(dx*dy)) #Generar el listado de número de píxeles\n",
    "\n",
    "#Generar un dataframe donde la primera columna sean el número de pixeles o características y la segundo el score:\n",
    "df = pd.DataFrame({'Pixeles': numPixel,'Score': Kbest.scores_.tolist()})\n",
    "\n",
    "lbpTrain = local_binary_pattern(np.reshape(train, (train.shape[0], dx*dy)), n_points, radius, METHOD)\n",
    "lbpTest = local_binary_pattern(np.reshape(test, (test.shape[0], dx*dy)), n_points, radius, METHOD)\n",
    "lbpVal = local_binary_pattern(np.reshape(val, (val.shape[0], dx*dy)), n_points, radius, METHOD)\n",
    "\n",
    "pixelesSelec = df[df.Score >= 200]['Pixeles'].values.tolist()\n",
    "X_train_tratada = Xtrain[:,pixelesSelec]\n",
    "X_test_tratada = Xtest[:,pixelesSelec]\n",
    "X_val_tratada = Xval[:,pixelesSelec]\n",
    "\n",
    "for x in c:\n",
    "    print(\"********************************* Prueba Regresion logistica ********************************* \")\n",
    "    LogReg = LogisticRegression(solver='liblinear', C=x, penalty='l1')\n",
    "    clasifica(LogReg, df, lbpTrain, lbpTest, lbpVal, y_train, y_test, y_val, score=[200], norm=True)\n",
    "    print(\"********************************* Prueba clasificador OVR ********************************* \")\n",
    "    OVR = OneVsRestClassifier(LogReg)\n",
    "    clasifica(OVR, df, lbpTrain, lbpTest, lbpVal, y_train, y_test, y_val, score=[200], norm=True)\n",
    "#Miramos la matriz de confusion para c=5\n",
    "LogReg = LogisticRegression(solver='liblinear', C=5, penalty='l1')\n",
    "clasifica(LogReg, df, lbpTrain, lbpTest, lbpVal, y_train, y_test, y_val, score=[200], norm=True)#OVR\n",
    "OVR = OneVsRestClassifier(LogReg)\n",
    "clasifica(OVR, df, lbpTrain, lbpTest, lbpVal, y_train, y_test, y_val, score=[200], norm=True)\n",
    "genera_confusion(LogReg,X_train_tratada,y_train)\n",
    "genera_confusion(LogReg,X_val_tratada,y_val)\n",
    "genera_confusion(LogReg,X_test_tratada,y_test)\n",
    "genera_confusion(OVR,X_train_tratada,y_train)\n",
    "genera_confusion(OVR,X_val_tratada,y_val)\n",
    "genera_confusion(OVR,X_test_tratada,y_test)\n",
    "\n",
    "############################################ Fin de pruebas con LBP ###########################################################\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
